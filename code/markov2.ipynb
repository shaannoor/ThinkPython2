{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\nThis module contains a code example related to<br>\n", "Think Python, 2nd Edition<br>\n", "by Allen Downey<br>\n", "http://thinkpython2.com<br>\n", "Copyright 2015 Allen Downey<br>\n", "License: http://creativecommons.org/licenses/by/4.0/<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import print_function, division"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "import random"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from markov import skip_gutenberg_header, shift"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Markov:\n", "    \"\"\"Encapsulates the statistical summary of a text.\"\"\"\n", "    def __init__(self):\n", "        self.suffix_map = {}        # map from prefixes to a list of suffixes\n", "        self.prefix = ()            # current tuple of words\n", "    def process_file(self, filename, order=2):\n", "        \"\"\"Reads a file and performs Markov analysis.\n", "        filename: string\n", "        order: integer number of words in the prefix\n", "        Returns: map from prefix to list of possible suffixes.\n", "        \"\"\"\n", "        fp = open(filename)\n", "        skip_gutenberg_header(fp)\n", "        for line in fp:\n", "            if line.startswith('*** END OF THIS'): \n", "                break\n", "            for word in line.rstrip().split():\n", "                self.process_word(word, order)\n", "    def process_word(self, word, order=2):\n", "        \"\"\"Processes each word.\n", "        word: string\n", "        order: integer\n", "        During the first few iterations, all we do is store up the words; \n", "        after that we start adding entries to the dictionary.\n", "        \"\"\"\n", "        if len(self.prefix) < order:\n", "            self.prefix += (word,)\n", "            return\n", "        try:\n", "            self.suffix_map[self.prefix].append(word)\n", "        except KeyError:\n", "            # if there is no entry for this prefix, make one\n", "            self.suffix_map[self.prefix] = [word]\n", "        self.prefix = shift(self.prefix, word)        \n", "    def random_text(self, n=100):\n", "        \"\"\"Generates random wordsfrom the analyzed text.\n", "        Starts with a random prefix from the dictionary.\n", "        n: number of words to generate\n", "        \"\"\"\n", "        # choose a random prefix (not weighted by frequency)\n", "        start = random.choice(list(self.suffix_map.keys()))\n", "        for i in range(n):\n", "            suffixes = self.suffix_map.get(start, None)\n", "            if suffixes == None:\n", "                # if the prefix isn't in map, we got to the end of the\n", "                # original text, so we have to start again.\n", "                self.random_text(n-i)\n", "                return\n\n", "            # choose a random suffix\n", "            word = random.choice(suffixes)\n", "            print(word, end=' ')\n", "            start = shift(start, word)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main(script, filename='158-0.txt', n=100, order=2):\n", "    try:\n", "        n = int(n)\n", "        order = int(order)\n", "    except ValueError:\n", "        print('Usage: %d filename [# of words] [prefix length]' % script)\n", "    else: \n", "        markov = Markov()\n", "        markov.process_file(filename, order)\n", "        markov.random_text(n)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}